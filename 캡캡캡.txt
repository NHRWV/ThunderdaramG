nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('tagsets')
tokenize(tokens_en)
from nltk.tag import pos_tag
tag_list = pos_tag(tokenize(tokens_en))


대본 자료를 보면 같은 대본 사이트에서 가져왔다고 해도 내부 형식이 조금씩 다름

우선 8개의 대본만 선별해서 샘플로 사용해봄
movie_script함수에서 대본을 열고 띄어쓰기와 알파벳을 기준으로 토큰화해서 불러들이기.

그 다음에 흥행여부에 따라서 대본을 합쳤습니다. 조금씩 다른 두 가지 방식으로 대본을 합침.
하나는 리스트 안에 영화마다 리스트를 따로 만들어서 넣는 경우고 또 다른 방식은 하나의 리스트에 모든 대본을 섞어서 그냥 넣은 경우로서 단순한 bag of words라 볼 수 있음. 젠심을 이용하는 경우 토픽모델링 이전에 딕셔너리와 코퍼스를 생성해야 하기 위해 전자의 방식을 이용했습니다.

특정대본들을 한 데 모으고 나서 다시 한 번 토큰화와 전처리를 진행
불용어 리스트에 등장인물의 이름을 추가.
사실 품사태깅으로 단어들을 구분해서 뽑아볼 시도도 했는데 생각보다 태깅이 원하는 대로 안됨.
e.g) general... standfordpostag도 시도해 봄. 왜 시도하냐면 행동분석도 할 수 있을까해서.


먼저 사이킷런을 이용한 방식은 토픽은 그렇다치고 일단 현재로서는 펄플렉서티가 많이 높음.
좀 더 알맞은 샘플을. 확보한 후에 옵션들을 조정해 볼 필요 있음

젠심을 이용한 케이스도 마찬가지. 샘플이 부족해서 그런가 애초에 토픽이 4개밖에 안뽑혀..
필요한 대본을 정제하는 작업이 진행 중이기에 작업이 끝나면 더 많은 대본 확보 가능.

현재로선 정제한 대본 개수를 빨리 늘려서 토픽모델링에 필요한 문서의 양을 늘리는 게 관건.


sentimentintensityanalyzer shap
